"""
VIVOHOME AI - Agent Core Module
ReAct Loop + State Management + Evaluator
"""

import requests
import re
import json
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from tools import (
    VLLM_URL, REASONING_MODEL, VISION_MODEL,
    get_tool_descriptions, execute_tool, clean_response, encode_image
)

# === AGENT STATE ===
@dataclass
class AgentState:
    """Qu·∫£n l√Ω tr·∫°ng th√°i c·ªßa Agent trong m·ªôt phi√™n l√†m vi·ªác"""
    messages: List[Dict] = field(default_factory=list)
    current_query: str = ""
    image_path: Optional[str] = None
    tool_calls: List[Dict] = field(default_factory=list)
    observations: List[str] = field(default_factory=list)
    final_answer: Optional[str] = None
    iteration: int = 0
    
    def estimate_tokens(self) -> int:
        """∆Ø·ªõc t√≠nh s·ªë tokens (1 token ‚âà 4 k√Ω t·ª± ti·∫øng Vi·ªát)"""
        total_chars = sum(len(str(m.get('content', ''))) for m in self.messages)
        total_chars += len(self.current_query)
        total_chars += sum(len(str(obs)) for obs in self.observations)
        return total_chars // 4
    
    def prune_if_needed(self, max_tokens: int = 1800):
        """C·∫Øt t·ªâa l·ªãch s·ª≠ n·∫øu v∆∞·ª£t qu√° token limit"""
        while self.estimate_tokens() > max_tokens and len(self.messages) > 2:
            # Gi·ªØ l·∫°i system prompt (index 0) v√† tin g·∫ßn nh·∫•t
            self.messages.pop(1)
        
        # C≈©ng c·∫Øt observations n·∫øu qu√° d√†i
        while len(self.observations) > 3:
            self.observations.pop(0)

# === REACT PROMPT TEMPLATE ===
REACT_SYSTEM_PROMPT = """B·∫°n l√† AI Agent c·ªßa VIVOHOME. H√£y suy nghƒ© t·ª´ng b∆∞·ªõc v√† s·ª≠ d·ª•ng c√¥ng c·ª• khi c·∫ßn thi·∫øt.

C√ÅC C√îNG C·ª§ C√ì S·∫¥N:
{tool_descriptions}

QUY TR√åNH REACT:
1. Thought: Ph√¢n t√≠ch y√™u c·∫ßu c·ªßa kh√°ch
2. Action: Ch·ªçn m·ªôt c√¥ng c·ª• ph√π h·ª£p (ho·∫∑c "FINISH" n·∫øu ƒë√£ ƒë·ªß th√¥ng tin)
3. Action Input: Tham s·ªë cho c√¥ng c·ª• (JSON format)

L∆ØU √ù:
- N·∫øu c√≥ ·∫£nh, ∆∞u ti√™n d√πng extract_model tr∆∞·ªõc ƒë·ªÉ l·∫•y m√£ Model
- Sau khi c√≥ Model, d√πng lookup_csv ƒë·ªÉ tra gi√°
- N·∫øu kh√¥ng c√≥ ·∫£nh v√† kh√°ch h·ªèi chung, d√πng search_products
- Khi ƒë√£ ƒë·ªß th√¥ng tin, Action = "FINISH" v√† ƒë∆∞a ra c√¢u tr·∫£ l·ªùi

FORMAT ƒê·∫¶U RA (B·∫ÆT BU·ªòC):
Thought: [suy nghƒ© c·ªßa b·∫°n]
Action: [t√™n_c√¥ng_c·ª• ho·∫∑c FINISH]
Action Input: {{"param": "value"}}
"""

def build_react_prompt(query: str, image_context: str = "", observations: List[str] = None) -> str:
    """X√¢y d·ª±ng prompt cho ReAct loop"""
    prompt = REACT_SYSTEM_PROMPT.format(tool_descriptions=get_tool_descriptions())
    prompt += f"\n\nC√ÇU H·ªéI C·ª¶A KH√ÅCH: {query}"
    
    if image_context:
        prompt += f"\n\n[C√ì ·∫¢NH ƒê√çNH K√àM: {image_context}]"
    
    if observations:
        prompt += "\n\nK·∫æT QU·∫¢ T·ª™ C√ÅC C√îNG C·ª§ ƒê√É D√ôNG:"
        for i, obs in enumerate(observations, 1):
            prompt += f"\n{i}. {obs}"
    
    prompt += "\n\nB·∫Øt ƒë·∫ßu suy nghƒ©:"
    return prompt

# === PARSE REACT OUTPUT ===
def parse_react_output(text: str) -> Dict[str, Any]:
    """Parse output c·ªßa LLM theo format ReAct"""
    result = {"thought": "", "action": "", "action_input": {}}
    
    # Clean response first
    text = clean_response(text)
    
    # Extract Thought
    thought_match = re.search(r'Thought:\s*(.+?)(?=Action:|$)', text, re.DOTALL | re.IGNORECASE)
    if thought_match:
        result["thought"] = thought_match.group(1).strip()
    
    # Extract Action
    action_match = re.search(r'Action:\s*(\w+)', text, re.IGNORECASE)
    if action_match:
        result["action"] = action_match.group(1).strip()
    
    # Extract Action Input
    input_match = re.search(r'Action Input:\s*(\{.+?\})', text, re.DOTALL)
    if input_match:
        try:
            result["action_input"] = json.loads(input_match.group(1))
        except json.JSONDecodeError:
            # Fallback: extract simple value
            simple_match = re.search(r'Action Input:\s*(.+?)(?=\n|$)', text)
            if simple_match:
                result["action_input"] = {"value": simple_match.group(1).strip()}
    
    return result

# === REACT AGENT ===
class ReActAgent:
    """ReAct Agent v·ªõi v√≤ng l·∫∑p Thought-Action-Observation"""
    
    def __init__(self, max_iterations: int = 3, max_tokens: int = 1800):
        self.max_iterations = max_iterations
        self.max_tokens = max_tokens
    
    def _call_llm(self, prompt: str, image_path: Optional[str] = None) -> str:
        """G·ªçi LLM (Vision ho·∫∑c Text)"""
        if image_path:
            # D√πng Vision model
            base64_img = encode_image(image_path)
            messages = [{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}}
                ]
            }]
            model = VISION_MODEL
        else:
            # D√πng Reasoning model
            messages = [{"role": "user", "content": prompt}]
            model = REASONING_MODEL
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.2,
            "max_tokens": 400
        }
        
        try:
            r = requests.post(VLLM_URL, json=payload, timeout=60)
            result = r.json()
            if 'choices' in result:
                return result['choices'][0]['message']['content']
        except Exception as e:
            return f"L·ªói LLM: {e}"
        
        return "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ LLM"
    
    def run(self, query: str, image_path: Optional[str] = None) -> str:
        """Ch·∫°y ReAct loop"""
        state = AgentState(current_query=query, image_path=image_path)
        
        image_context = "C√≥ ·∫£nh ƒë√≠nh k√®m, h√£y d√πng extract_model ƒë·ªÉ l·∫•y m√£ Model" if image_path else ""
        
        for iteration in range(self.max_iterations):
            state.iteration = iteration + 1
            state.prune_if_needed(self.max_tokens)
            
            # Build prompt
            prompt = build_react_prompt(query, image_context, state.observations)
            
            # Call LLM (ch·ªâ d√πng image ·ªü l·∫ßn ƒë·∫ßu n·∫øu c·∫ßn describe)
            llm_response = self._call_llm(prompt, image_path=None)
            
            # Parse response
            parsed = parse_react_output(llm_response)
            
            # Check if FINISH
            if parsed["action"].upper() == "FINISH" or not parsed["action"]:
                # Generate final answer
                state.final_answer = self._generate_final_answer(state, parsed["thought"])
                break
            
            # Execute tool
            action_input = parsed["action_input"]
            
            # Handle image path for vision tools
            if parsed["action"] in ["extract_model", "describe_image"] and image_path:
                action_input["image_path"] = image_path
            elif parsed["action"] == "lookup_csv" and "model_code" in action_input:
                pass  # Already has model_code
            elif parsed["action"] == "search_products" and "query" in action_input:
                pass  # Already has query
            elif parsed["action"] == "lookup_csv" and "value" in action_input:
                action_input = {"model_code": action_input["value"]}
            elif parsed["action"] == "search_products" and "value" in action_input:
                action_input = {"query": action_input["value"]}
            
            # Execute
            tool_result = execute_tool(parsed["action"], **action_input)
            
            # Record
            state.tool_calls.append({"action": parsed["action"], "input": action_input})
            observation = f"[{parsed['action']}] ‚Üí {json.dumps(tool_result, ensure_ascii=False)}"
            state.observations.append(observation)
        
        # If we exhausted iterations without FINISH
        if not state.final_answer:
            state.final_answer = self._generate_final_answer(state, "ƒê√£ th·ª≠ nhi·ªÅu l·∫ßn")
        
        return state.final_answer
    
    def _generate_final_answer(self, state: AgentState, last_thought: str) -> str:
        """T·∫°o c√¢u tr·∫£ l·ªùi cu·ªëi c√πng t·ª´ observations"""
        # T√¨m th√¥ng tin s·∫£n ph·∫©m t·ª´ observations
        product_info = None
        for obs in state.observations:
            if '"found": true' in obs.lower() or '"found":true' in obs.lower():
                try:
                    # Extract JSON from observation
                    json_match = re.search(r'\{.*\}', obs)
                    if json_match:
                        data = json.loads(json_match.group())
                        if data.get("found") and data.get("ten_san_pham"):
                            product_info = data
                            break
                        elif data.get("found") and data.get("products"):
                            product_info = data
                            break
                except:
                    pass
        
        if product_info:
            if "ten_san_pham" in product_info:
                return f"""üì¶ **Th√¥ng tin s·∫£n ph·∫©m:**
- T√™n: {product_info['ten_san_pham']}
- Model: {product_info['model']}
- Gi√°: **{product_info['gia']:,} VND**
- Nh√≥m: {product_info.get('nhom_hang', 'N/A')}"""
            elif "products" in product_info:
                lines = ["üì¶ **S·∫£n ph·∫©m t√¨m ƒë∆∞·ª£c:**"]
                for p in product_info["products"]:
                    lines.append(f"- {p['ten']} ({p['model']}): **{p['gia']:,} VND**")
                return "\n".join(lines)
        
        # Fallback: summarize observations
        if state.observations:
            return f"D·ª±a tr√™n th√¥ng tin t√¨m ƒë∆∞·ª£c:\n" + "\n".join(state.observations[-2:])
        
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ s·∫£n ph·∫©m n√†y trong h·ªá th·ªëng VIVOHOME."

# === MODEL-BASED EVALUATOR ===
def evaluate_response(question: str, ai_answer: str, ground_truth: Optional[Dict] = None) -> Dict:
    """
    D√πng DeepSeek l√†m gi√°m kh·∫£o ch·∫•m ƒëi·ªÉm c√¢u tr·∫£ l·ªùi.
    Returns: {"score": 1-5, "feedback": str}
    """
    eval_prompt = f"""Ch·∫•m ƒëi·ªÉm c√¢u tr·∫£ l·ªùi c·ªßa AI b√°n h√†ng (thang 1-5).

C√¢u h·ªèi kh√°ch: {question}
C√¢u tr·∫£ l·ªùi AI: {ai_answer}
{"D·ªØ li·ªáu th·ª±c: " + str(ground_truth) if ground_truth else ""}

Ti√™u ch√≠:
1 = Sai ho√†n to√†n
2 = Sai m·ªôt ph·∫ßn
3 = ƒê√∫ng nh∆∞ng thi·∫øu
4 = ƒê√∫ng v√† ƒë·∫ßy ƒë·ªß
5 = Xu·∫•t s·∫Øc

Tr·∫£ l·ªùi format: SCORE: [s·ªë] | FEEDBACK: [nh·∫≠n x√©t ng·∫Øn]"""

    try:
        payload = {
            "model": REASONING_MODEL,
            "messages": [{"role": "user", "content": eval_prompt}],
            "temperature": 0.1,
            "max_tokens": 100
        }
        r = requests.post(VLLM_URL, json=payload, timeout=30)
        result = r.json()
        
        if 'choices' in result:
            text = clean_response(result['choices'][0]['message']['content'])
            score_match = re.search(r'SCORE:\s*(\d)', text)
            feedback_match = re.search(r'FEEDBACK:\s*(.+)', text)
            
            return {
                "score": int(score_match.group(1)) if score_match else 3,
                "feedback": feedback_match.group(1).strip() if feedback_match else text
            }
    except:
        pass
    
    return {"score": 0, "feedback": "Kh√¥ng th·ªÉ ƒë√°nh gi√°"}

# === TEST ===
if __name__ == "__main__":
    agent = ReActAgent(max_iterations=3)
    
    # Test 1: Text query
    print("=" * 50)
    print("TEST 1: Text query")
    result = agent.run("B√¨nh t·∫Øm Rossi 15 l√≠t gi√° bao nhi√™u?")
    print(result)
    
    # Evaluate
    eval_result = evaluate_response(
        "B√¨nh t·∫Øm Rossi 15 l√≠t gi√° bao nhi√™u?",
        result,
        {"gia": 1500000}
    )
    print(f"\nüìä Evaluation: Score={eval_result['score']}/5 | {eval_result['feedback']}")
